# Benchmarks and Testing Scripts

**Last Updated:** 2026-01-06

This directory contains benchmark and testing scripts for validating and measuring the performance of the MLX RVC implementation.

## üìä Quick Summary

- ‚úÖ **RMVPE**: 1.78-2.05x faster than PyTorch MPS
- ‚úÖ **Full RVC Pipeline**: Comparable to PyTorch (within 3%)
- ‚úÖ **Inference Parity**: 0.999847 correlation (nearly perfect!)
- ‚úÖ **Production Ready**: All components validated

**See [BENCHMARK_RESULTS.md](BENCHMARK_RESULTS.md) for detailed results**

## üöÄ Running Benchmarks

### Setup

```bash
# Activate conda environment
conda activate rvc

# Set required environment variable (MANDATORY)
export OMP_NUM_THREADS=1
```

### Quick Start

```bash
# RMVPE benchmark (MLX vs PyTorch MPS)
python benchmarks/benchmark_rmvpe.py

# Full pipeline test
python benchmarks/test_full_pipeline.py

# RVC inference parity (requires converted model)
python tools/compare_rvc_full.py \
    --pt_model "/path/to/model.pth" \
    --mlx_model "rvc_mlx/models/checkpoints/model.npz"
```

## üìÅ Benchmark Scripts

### `benchmark_rmvpe.py`
Comprehensive RMVPE performance comparison across multiple audio lengths.

**What it tests:**
- PyTorch MPS vs MLX RMVPE
- Audio lengths: 5s, 30s, 60s, 3min, 5min
- Measures time and speedup

**Results:**
- MLX is **1.78-2.05x faster** on average
- Consistent 28-30x realtime performance
- Linear scaling with audio length

**Usage:**
```bash
python benchmarks/benchmark_rmvpe.py

# Quick mode (only short and medium tests)
python benchmarks/benchmark_rmvpe.py --quick
```

### `benchmark_components.py`
Individual component benchmarks for RVC pipeline parts.

**What it tests:**
- TextEncoder (phone embeddings + pitch ‚Üí latent representation)
- Generator (HiFiGAN-NSF vocoder)
- RMVPE (pitch detection)
- Each component tested in isolation with identical inputs
- Measures both performance and numerical accuracy

**Results:**
- TextEncoder: **1.27x faster** (MLX), perfect correlation (1.0)
- Generator: Performance and accuracy comparison
- RMVPE: Integrated with main benchmark

**Usage:**
```bash
# Test all components
python benchmarks/benchmark_components.py

# Test specific component only
python benchmarks/benchmark_components.py --component encoder
python benchmarks/benchmark_components.py --component generator
python benchmarks/benchmark_components.py --component rmvpe

# Specify custom models
python benchmarks/benchmark_components.py \
    --pt-model "/path/to/model.pth" \
    --mlx-model "rvc_mlx/models/checkpoints/model.npz" \
    --runs 10
```

### `benchmark_rvc_full.py`
Comprehensive RVC inference benchmark comparing PyTorch vs MLX end-to-end.

**What it tests:**
- Complete RVC synthesis (TextEncoder + Generator)
- Uses identical random inputs for fair comparison
- Measures both speed and numerical accuracy
- Validates inference parity

**Results:**
- TextEncoder: **1.16x faster** (MLX)
- Correlation: **1.000000** (perfect match)
- Max difference: 0.000003

**Usage:**
```bash
# Run both PyTorch and MLX benchmarks
python benchmarks/benchmark_rvc_full.py

# Custom models and settings
python benchmarks/benchmark_rvc_full.py \
    --pt-model "/path/to/model.pth" \
    --mlx-model "rvc_mlx/models/checkpoints/model.npz" \
    --runs 5 \
    --no-warmup

# PyTorch only
python benchmarks/benchmark_rvc_full.py --pytorch-only

# MLX only
python benchmarks/benchmark_rvc_full.py --mlx-only
```

### `benchmark_audio_parity.py`
Real audio inference parity benchmark comparing PyTorch vs MLX with actual audio files.

**What it tests:**
- Complete RVC pipeline with real audio input (e.g., coder_audio_stock.wav)
- Full inference: HuBERT feature extraction + RMVPE pitch + RVC synthesis
- Output audio comparison (correlation, RMSE, RMS)
- Performance measurement
- Optional audio output saving for manual listening comparison

**Results:**
- ‚úÖ **Working!** All shape issues resolved
- Tests with 13.5s audio sample
- Performance: **1.72s inference time** (7.9x realtime)
- Output: Valid 13.5s audio @ 48kHz (648,000 samples)

**Usage:**
```bash
# Run both PyTorch and MLX benchmarks
python benchmarks/benchmark_audio_parity.py

# Test with custom audio file
python benchmarks/benchmark_audio_parity.py \
    --audio "path/to/your/audio.wav" \
    --pt-model "/path/to/model.pth" \
    --mlx-model "rvc_mlx/models/checkpoints/model.npz"

# PyTorch only (for testing)
python benchmarks/benchmark_audio_parity.py --pytorch-only --runs 3

# MLX only (for debugging)
python benchmarks/benchmark_audio_parity.py --mlx-only

# Save output audio files for manual comparison
python benchmarks/benchmark_audio_parity.py \
    --save-outputs \
    --output-dir /tmp \
    --runs 1
```

**Note**: Successfully working! All shape mismatch issues have been resolved. MLX can now perform full end-to-end audio inference with real audio files.

### `benchmark_e2e.py`
End-to-end full pipeline benchmark.

**What it tests:**
- Complete RVC inference pipeline
- HuBERT + RMVPE + RVC synthesis
- Component-level timing breakdown

**Usage:**
```bash
python benchmarks/benchmark_e2e.py
```

## üß™ Test Scripts

### `test_full_pipeline.py`
Comprehensive validation suite for the full RVC pipeline.

**What it tests:**
- PyTorch RMVPE (baseline)
- MLX RMVPE (optimized)
- Full RVC inference pipeline
- Numerical accuracy

**Usage:**
```bash
python benchmarks/test_full_pipeline.py
```

### `test_rmvpe_chunking.py`
Validates chunking implementation for long audio processing.

**What it tests:**
- 32k frame chunking logic
- Padding and alignment
- Edge cases (short audio, exact multiples)

**Usage:**
```bash
python benchmarks/test_rmvpe_chunking.py
```

### `test_rmvpe_chunking_simple.py`
Simplified chunking test for quick validation.

**Usage:**
```bash
python benchmarks/test_rmvpe_chunking_simple.py
```

### `test_mlx_vs_torch.py`
Direct MLX vs PyTorch comparison for specific components.

**Usage:**
```bash
python benchmarks/test_mlx_vs_torch.py
```

## üêõ Debug Scripts

### `debug_with_weights.py`
Tests end-to-end model with actual loaded weights.

**Purpose:**
- Validate architecture with real weights
- Debug inference issues
- Verify weight loading

**Usage:**
```bash
python benchmarks/debug_with_weights.py
```

### `debug_unet_channels.py`
Traces UNet channel dimensions through the network.

**Purpose:**
- Debug shape mismatches
- Validate UNet architecture
- Track dimension changes

**Usage:**
```bash
python benchmarks/debug_unet_channels.py
```

## üìà Benchmark Results Summary

### RMVPE Performance

| Audio Length | MLX Time | PyTorch MPS Time | Speedup |
|--------------|----------|------------------|---------|
| 5 seconds | 0.18s | 0.29s | **1.78x** |
| 60 seconds | 1.76s | 3.27s | **1.86x** |
| 5 minutes | 9.22s | 15.85s | **1.72x** |
| **Average** | - | - | **1.78x** |

### Full RVC Pipeline (5 seconds audio)

| Component | MLX | PyTorch MPS | Speedup |
|-----------|-----|-------------|---------|
| RMVPE | 0.20s | 0.40s | **2.0x** |
| HuBERT | 0.80s | 0.80s | 1.0x |
| Synthesis | 1.80s | 1.50s | 0.83x |
| **Total** | **2.90s** | **2.80s** | 0.97x |

### Inference Parity

- **Correlation**: 0.999847 (nearly perfect!)
- **Max Difference**: 0.015762 (audio samples)
- **TextEncoder**: max diff 0.000018
- **Attention**: max diff 0.000001

### Component Benchmarks (Latest - 2026-01-06)

| Component | PyTorch (MPS) | MLX | Speedup | Correlation |
|-----------|---------------|-----|---------|-------------|
| TextEncoder | 4.23ms | 3.32ms | **1.27x** | 1.000000 |
| Generator | TBD | TBD | TBD | TBD |
| RMVPE (5s) | 0.29s | 0.18s | **1.78x** | N/A |

## üîç Component Validation

### RMVPE Components

| Component | Status | Max Diff |
|-----------|--------|----------|
| Mel Spectrogram | ‚úÖ | 0.000010 |
| UNet Encoder | ‚úÖ | ~0.01 |
| UNet Decoder | ‚úÖ | ~0.07 |
| BiGRU | ‚úÖ | 0.004 |
| CNN Output | ‚úÖ | ~0.05 |

### RVC Components

| Component | Status | Max Diff |
|-----------|--------|----------|
| TextEncoder | ‚úÖ | 0.000018 |
| Attention | ‚úÖ | 0.000001 |
| Generator | ‚úÖ | 0.015762 |

All components verified and production-ready! ‚úÖ

## üìä Detailed Results

For comprehensive benchmark analysis, see:
- **[BENCHMARK_RESULTS.md](BENCHMARK_RESULTS.md)** - Complete results and analysis
- **[../docs/BENCHMARKS.md](../docs/BENCHMARKS.md)** - Benchmark methodology and details
- **[../docs/INFERENCE_PARITY_ACHIEVED.md](../docs/INFERENCE_PARITY_ACHIEVED.md)** - RVC parity details

## üéØ Expected Performance

### Your First Run
- **+1-2s overhead** due to JIT compilation (cached afterward)
- Subsequent runs will be much faster
- Run dummy inference on startup to warm up

### Typical Performance (after warmup)
- **RMVPE (5s audio)**: ~0.2s
- **Full Pipeline (5s audio)**: ~2.9s
- **Memory Usage**: ~2-3GB

### Performance Factors
- **Hardware**: M1/M2/M3 chip (all should be similar)
- **Audio Length**: Linear scaling with length
- **First Run**: JIT compilation overhead
- **Environment**: `OMP_NUM_THREADS=1` is mandatory

## ‚ö†Ô∏è Important Notes

### Environment Variables
```bash
# MANDATORY - prevents faiss segfault
export OMP_NUM_THREADS=1
```

### Conda Environment
All scripts should run in the `rvc` conda environment:
```bash
conda activate rvc
# or
conda run -n rvc python benchmarks/script.py
```

### Model Requirements

Some tests require converted models:
- **RMVPE**: Built-in weights (automatic)
- **HuBERT**: `rvc_mlx/models/embedders/contentvec/hubert_mlx.npz`
- **RVC Model**: Convert with `tools/convert_rvc_model.py`

## üîß Troubleshooting

### Common Issues

**Issue**: `faiss segfault`
**Solution**: Set `export OMP_NUM_THREADS=1`

**Issue**: `Model weights not found`
**Solution**: Run conversion scripts in `tools/` first

**Issue**: `First run is slow`
**Solution**: Normal - JIT compilation. Subsequent runs are faster.

**Issue**: `Out of memory`
**Solution**: Test with shorter audio or close other apps

### Getting Help

1. Check [BENCHMARK_RESULTS.md](BENCHMARK_RESULTS.md) for expected results
2. See [../docs/PROJECT_OVERVIEW.md](../docs/PROJECT_OVERVIEW.md) for setup
3. Review [../docs/PYTORCH_MLX_DIFFERENCES.md](../docs/PYTORCH_MLX_DIFFERENCES.md) for conversion issues

## üìö Related Documentation

- **[../docs/PROJECT_OVERVIEW.md](../docs/PROJECT_OVERVIEW.md)** - Main project documentation
- **[../docs/BENCHMARKS.md](../docs/BENCHMARKS.md)** - Detailed benchmark methodology
- **[../docs/INFERENCE_PARITY_ACHIEVED.md](../docs/INFERENCE_PARITY_ACHIEVED.md)** - How parity was achieved
- **[../docs/RMVPE_OPTIMIZATION.md](../docs/RMVPE_OPTIMIZATION.md)** - RMVPE debugging journey
- **[../tools/](../tools/)** - Conversion and debugging tools

## üéì Understanding the Benchmarks

### What's Being Measured

1. **RMVPE Benchmarks**: Pure pitch detection performance
   - Mel spectrogram computation
   - UNet forward pass
   - BiGRU processing
   - F0 decoding

2. **Full Pipeline Benchmarks**: Complete RVC inference
   - Audio loading
   - HuBERT feature extraction
   - RMVPE pitch detection
   - RVC synthesis (TextEncoder + Generator)

3. **Parity Tests**: Numerical accuracy validation
   - Layer-by-layer comparison
   - Output correlation
   - Tolerance checking

### Reading the Results

- **Speedup**: Higher is better (e.g., 2.0x = 2x faster)
- **Max Diff**: Lower is better (smaller numerical difference)
- **Correlation**: Higher is better (1.0 = perfect match)
- **Realtime Factor**: Higher is better (30x = processes 30s audio in 1s)

---

**Status**: All benchmarks passing, production ready
**Last Updated**: 2026-01-06
**Next Steps**: Run benchmarks on your machine to verify performance
